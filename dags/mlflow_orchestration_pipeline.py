from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.datasets import Dataset
import os
import json
import mlflow
import subprocess
import requests

# Dataset ì •ì˜
training_data_dataset = Dataset("file:///data/train/")
model_dataset = Dataset("mlflow://models/image-classification-cnn/latest")
inference_results_dataset = Dataset("file:///data/inference_results.json")

default_args = {
    'owner': 'ml-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'mlflow_orchestration_pipeline',
    default_args=default_args,
    description='MLflow-based ML pipeline orchestration with manual BentoML deployment',
    schedule=timedelta(hours=6),
    catchup=False,
    tags=['mlflow', 'pytorch', 'image-classification', 'orchestration', 'bentoml'],
)

def generate_sample_data(**context):
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    print("ðŸŽ¨ Generating sample images...")
    result = subprocess.run([
        'python', '/opt/airflow/data/generate_sample_data.py'
    ], capture_output=True, text=True, cwd='/opt/airflow')
    
    if result.returncode == 0:
        print("âœ… Sample data generated successfully")
    else:
        raise Exception(f"Data generation failed: {result.stderr}")

def trigger_mlflow_training(**context):
    """MLflow í›ˆë ¨ ìž‘ì—… íŠ¸ë¦¬ê±°"""
    print("ðŸš€ Triggering MLflow training...")
    
    # MLflow ì„¤ì •
    mlflow.set_tracking_uri("http://mlflow-server:5000")
    
    # MLflow Training ì„œë¹„ìŠ¤ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
    import time
    
    max_retries = 30  # ìµœëŒ€ 15ë¶„ ëŒ€ê¸°
    for attempt in range(max_retries):
        try:
            health_response = requests.get("http://mlflow-training:8000/health", timeout=10)
            if health_response.status_code == 200:
                print("âœ… MLflow Training service is ready!")
                break
        except requests.exceptions.ConnectionError:
            pass
        
        print(f"â³ Waiting for MLflow Training service... (attempt {attempt + 1}/{max_retries})")
        time.sleep(30)  # 30ì´ˆ ëŒ€ê¸°
    else:
        raise Exception("MLflow Training service did not start within 15 minutes")
    
    try:
        training_request = {"data_path": "/data", "epochs": 10, "learning_rate": 0.001, "batch_size": 4}
        response = requests.post("http://mlflow-training:8000/train", json=training_request, timeout=1800)
        
        if response.status_code == 200:
            result_data = response.json()
            print(f"âœ… MLflow training completed successfully: {result_data}")
            
            # XComì— ê²°ê³¼ ì €ìž¥
            context['ti'].xcom_push(key='training_status', value='success')
            context['ti'].xcom_push(key='run_id', value=result_data.get('run_id'))
            
            return result_data
        else:
            raise Exception(f"Training failed: {response.json().get('detail', 'Unknown error')}")
    except requests.exceptions.Timeout:
        raise Exception("Training timed out after 30 minutes")
    except requests.exceptions.ConnectionError:
        raise Exception("Could not connect to MLflow training service")
    except Exception as e:
        raise Exception(f"Training execution error: {str(e)}")

def get_latest_model_info(**context):
    """ìµœì‹  ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    print("ðŸ“‹ Getting latest model information...")
    
    mlflow.set_tracking_uri("http://mlflow-server:5000")
    
    try:
        client = mlflow.tracking.MlflowClient()
        model_name = "image-classification-cnn"
        
        # ìµœì‹  ëª¨ë¸ ë²„ì „ ì¡°íšŒ
        latest_versions = client.get_latest_versions(
            model_name, 
            stages=["Production", "Staging", "None"]
        )
        
        if latest_versions:
            latest_version = latest_versions[0]
            model_uri = f"models:/{model_name}/{latest_version.version}"
            
            # ëª¨ë¸ ì •ë³´
            model_info = {
                'model_name': model_name,
                'version': latest_version.version,
                'stage': latest_version.current_stage,
                'model_uri': model_uri,
                'run_id': latest_version.run_id
            }
            
            print(f"ðŸ“ Latest model: {model_uri}")
            
            # XComì— ì €ìž¥
            context['ti'].xcom_push(key='model_info', value=model_info)
            
            return model_info
        else:
            raise Exception("No model versions found")
    except Exception as e:
        raise Exception(f"Failed to get model info: {str(e)}")

def trigger_mlflow_inference(**context):
    """MLflow ì¶”ë¡  ìž‘ì—… íŠ¸ë¦¬ê±°"""
    print("ðŸ”® Triggering MLflow inference...")
    
    model_info = context['ti'].xcom_pull(task_ids='get_model_info', key='model_info')
    
    if not model_info:
        raise Exception("Model information not available")
    
    try:
        response = requests.post(
            "http://mlflow-training:8000/inference",
            params={"model_uri": model_info['model_uri'], "data_path": "/data/test/images"},
            timeout=600
        )
        
        if response.status_code == 200:
            result_data = response.json()
            print(f"âœ… MLflow inference completed successfully: {result_data}")
            summary = result_data.get('results', {})
            context['ti'].xcom_push(key='inference_summary', value=summary)
        else:
            raise Exception(f"Inference failed: {response.json().get('detail', 'Unknown error')}")
    except requests.exceptions.Timeout:
        raise Exception("Inference timed out after 10 minutes")
    except requests.exceptions.ConnectionError:
        raise Exception("Could not connect to MLflow inference service")
    except Exception as e:
        raise Exception(f"Inference execution error: {str(e)}")

def validate_pipeline(**context):
    """íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ê²€ì¦ (ë°°í¬ ì œì™¸)"""
    print("âœ… Validating pipeline results...")
    training_status = context['ti'].xcom_pull(task_ids='trigger_training', key='training_status')
    model_info = context['ti'].xcom_pull(task_ids='get_model_info', key='model_info')
    inference_summary = context['ti'].xcom_pull(task_ids='trigger_inference', key='inference_summary')
    
    validation_results = {
        'training_completed': training_status == 'success',
        'model_available': model_info is not None,
        'inference_completed': inference_summary is not None,
        'pipeline_success': True
    }
    
    if not all(k for k in [validation_results['training_completed'], validation_results['model_available'], validation_results['inference_completed']]):
        validation_results['pipeline_success'] = False
        print(f"âŒ Pipeline validation failed. Results: {validation_results}")
        raise Exception("Pipeline validation failed")
    
    print("ðŸŽ‰ Pre-deployment validation successful!")
    return validation_results

# Task ì •ì˜
generate_data_task = PythonOperator(
    task_id='generate_sample_data',
    python_callable=generate_sample_data,
    outlets=[training_data_dataset],
    dag=dag,
)

train_task = PythonOperator(
    task_id='trigger_training',
    python_callable=trigger_mlflow_training,
    inlets=[training_data_dataset],
    outlets=[model_dataset],
    dag=dag,
)

get_model_task = PythonOperator(
    task_id='get_model_info',
    python_callable=get_latest_model_info,
    dag=dag,
)

inference_task = PythonOperator(
    task_id='trigger_inference',
    python_callable=trigger_mlflow_inference,
    inlets=[model_dataset],
    outlets=[inference_results_dataset],
    dag=dag,
)

manual_deployment_instructions = BashOperator(
    task_id='manual_deployment_instructions',
    bash_command=(
        "echo '\n' && "
        "echo '################################################################################' && "
        "echo 'âœ… Model training and validation complete.' && "
        "echo 'ðŸ‘‰ To deploy this model with BentoML, please run the following command in your shell:' && "
        "echo '   docker-compose exec airflow bash -c \"python /opt/airflow/scripts/deploy_with_bentoml.py\"' && "
        "echo '################################################################################' && "
        "echo '\n'"
    ),
    dag=dag,
)

validate_task = PythonOperator(
    task_id='validate_pipeline',
    python_callable=validate_pipeline,
    dag=dag,
)

# Task ì˜ì¡´ì„± ì •ì˜
generate_data_task >> train_task >> get_model_task >> inference_task >> validate_task >> manual_deployment_instructions