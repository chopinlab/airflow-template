from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.datasets import Dataset
import os
import json
import mlflow

# Dataset ì •ì˜
training_data_dataset = Dataset("file:///data/train/")
model_dataset = Dataset("mlflow://models/image-classification-cnn/latest")
inference_results_dataset = Dataset("file:///data/inference_results.json")

default_args = {
    'owner': 'ml-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'mlflow_orchestration_pipeline',
    default_args=default_args,
    description='MLflow-based ML pipeline orchestration with manual BentoML deployment',
    schedule=timedelta(hours=6),
    catchup=False,
    tags=['mlflow', 'pytorch', 'image-classification', 'orchestration', 'bentoml'],
)

def generate_sample_data(**context):
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    print("ðŸŽ¨ Generating sample images...")
    
    try:
        # Python ëª¨ë“ˆë¡œ ì§ì ‘ importí•´ì„œ ì‹¤í–‰
        import sys
        sys.path.append('/opt/airflow/data')
        
        # generate_sample_data ëª¨ë“ˆ ì‹¤í–‰
        import generate_sample_data
        
        # ëª¨ë“ˆì— main í•¨ìˆ˜ê°€ ìžˆë‹¤ë©´ ì‹¤í–‰, ì—†ë‹¤ë©´ ìŠ¤í¬ë¦½íŠ¸ ìžì²´ê°€ ì‹¤í–‰ë¨
        if hasattr(generate_sample_data, 'main'):
            generate_sample_data.main()
        
        print("âœ… Sample data generated successfully")
        
    except ImportError as e:
        print(f"âš ï¸ Could not import data generation script: {e}")
        # í´ë°±: ê°„ë‹¨í•œ ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
        data_dir = '/opt/airflow/data/train/images'
        if not os.path.exists(data_dir):
            os.makedirs(data_dir, exist_ok=True)
        print("âœ… Data directories ensured to exist")
        
    except Exception as e:
        raise Exception(f"Data generation failed: {str(e)}")

def trigger_mlflow_training(**context):
    """MLflow Projectsë¥¼ í†µí•œ í›ˆë ¨ ìž‘ì—… ì‹¤í–‰"""
    print("ðŸš€ Running MLflow Project for image classification training...")
    
    # MLflow ì„¤ì •
    mlflow.set_tracking_uri("http://mlflow-server:5000")
    
    try:
        # MLflow Projects ì‹¤í–‰ - ì´ë¯¸ì§€ ë¶„ë¥˜ í›ˆë ¨
        print("ðŸ“Š Starting MLflow Project execution...")
        
        submitted_run = mlflow.run(
            uri="/opt/airflow/mlflow-projects/image-classification",
            entry_point="train",
            parameters={
                "data_path": "/data",
                "epochs": 10,
                "learning_rate": 0.001,
                "batch_size": 4
            },
            experiment_name="image-classification",
            synchronous=True,  # ë™ê¸° ì‹¤í–‰ìœ¼ë¡œ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
            backend="docker",
            backend_config={
                "image": "mlflow-pytorch:latest",
                "volumes": {
                    "/opt/airflow/mlflow-projects": "/mlflow-projects",
                    "/opt/airflow/data": "/data"
                }
            }
        )
        
        run_id = submitted_run.run_id
        print(f"âœ… MLflow Project training completed successfully!")
        print(f"ðŸƒ Run ID: {run_id}")
        
        # ì‹¤í–‰ ê²°ê³¼ ì¡°íšŒ
        client = mlflow.tracking.MlflowClient()
        run_info = client.get_run(run_id)
        
        # ë©”íŠ¸ë¦­ ì¡°íšŒ
        final_accuracy = run_info.data.metrics.get('accuracy', 0)
        print(f"ðŸ“Š Final accuracy: {final_accuracy:.4f}")
        
        # XComì— ê²°ê³¼ ì €ìž¥
        context['ti'].xcom_push(key='training_status', value='success')
        context['ti'].xcom_push(key='run_id', value=run_id)
        context['ti'].xcom_push(key='accuracy', value=final_accuracy)
        
        return {
            'status': 'success',
            'run_id': run_id,
            'accuracy': final_accuracy
        }
        
    except Exception as e:
        print(f"âŒ MLflow Project execution failed: {str(e)}")
        context['ti'].xcom_push(key='training_status', value='failed')
        raise Exception(f"MLflow Project training failed: {str(e)}")

def get_latest_model_info(**context):
    """ìµœì‹  ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    print("ðŸ“‹ Getting latest model information...")
    
    mlflow.set_tracking_uri("http://mlflow-server:5000")
    
    try:
        client = mlflow.tracking.MlflowClient()
        model_name = "image-classification-cnn"
        
        # ìµœì‹  ëª¨ë¸ ë²„ì „ ì¡°íšŒ
        latest_versions = client.get_latest_versions(
            model_name, 
            stages=["Production", "Staging", "None"]
        )
        
        if latest_versions:
            latest_version = latest_versions[0]
            model_uri = f"models:/{model_name}/{latest_version.version}"
            
            # ëª¨ë¸ ì •ë³´
            model_info = {
                'model_name': model_name,
                'version': latest_version.version,
                'stage': latest_version.current_stage,
                'model_uri': model_uri,
                'run_id': latest_version.run_id
            }
            
            print(f"ðŸ“ Latest model: {model_uri}")
            
            # XComì— ì €ìž¥
            context['ti'].xcom_push(key='model_info', value=model_info)
            
            return model_info
        else:
            raise Exception("No model versions found")
    except Exception as e:
        raise Exception(f"Failed to get model info: {str(e)}")

def trigger_mlflow_inference(**context):
    """MLflow Projectsë¥¼ í†µí•œ ì¶”ë¡  ìž‘ì—… ì‹¤í–‰"""
    print("ðŸ”® Running MLflow Project for image classification inference...")
    
    model_info = context['ti'].xcom_pull(task_ids='get_model_info', key='model_info')
    
    if not model_info:
        raise Exception("Model information not available")
    
    # MLflow ì„¤ì •
    mlflow.set_tracking_uri("http://mlflow-server:5000")
    
    try:
        print(f"ðŸŽ¯ Using model: {model_info['model_uri']}")
        
        # MLflow Projects ì‹¤í–‰ - ì´ë¯¸ì§€ ë¶„ë¥˜ ì¶”ë¡ 
        submitted_run = mlflow.run(
            uri="/opt/airflow/mlflow-projects/image-classification",
            entry_point="inference",
            parameters={
                "model_uri": model_info['model_uri'],
                "data_path": "/data/test",
                "output_path": "/data/inference_results.json"
            },
            experiment_name="image-classification-inference",
            synchronous=True,
            backend="docker",
            backend_config={
                "image": "mlflow-pytorch:latest",
                "volumes": {
                    "/opt/airflow/mlflow-projects": "/mlflow-projects",
                    "/opt/airflow/data": "/data"
                }
            }
        )
        
        run_id = submitted_run.run_id
        print(f"âœ… MLflow Project inference completed successfully!")
        print(f"ðŸƒ Inference Run ID: {run_id}")
        
        # ì¶”ë¡  ê²°ê³¼ íŒŒì¼ ì½ê¸°
        try:
            import json
            with open('/opt/airflow/data/inference_results.json', 'r') as f:
                inference_results = json.load(f)
            
            summary = {
                'total_samples': inference_results.get('total_samples', 0),
                'accuracy': inference_results.get('accuracy', 0),
                'run_id': run_id
            }
            
            print(f"ðŸ“Š Inference summary: {summary}")
            context['ti'].xcom_push(key='inference_summary', value=summary)
            
            return summary
            
        except Exception as e:
            print(f"âš ï¸ Could not read inference results file: {e}")
            # ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
            summary = {'run_id': run_id, 'status': 'completed'}
            context['ti'].xcom_push(key='inference_summary', value=summary)
            return summary
        
    except Exception as e:
        print(f"âŒ MLflow Project inference failed: {str(e)}")
        raise Exception(f"MLflow Project inference failed: {str(e)}")

def validate_pipeline(**context):
    """íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ê²€ì¦ (ë°°í¬ ì œì™¸)"""
    print("âœ… Validating pipeline results...")
    training_status = context['ti'].xcom_pull(task_ids='trigger_training', key='training_status')
    model_info = context['ti'].xcom_pull(task_ids='get_model_info', key='model_info')
    inference_summary = context['ti'].xcom_pull(task_ids='trigger_inference', key='inference_summary')
    
    validation_results = {
        'training_completed': training_status == 'success',
        'model_available': model_info is not None,
        'inference_completed': inference_summary is not None,
        'pipeline_success': True
    }
    
    if not all(k for k in [validation_results['training_completed'], validation_results['model_available'], validation_results['inference_completed']]):
        validation_results['pipeline_success'] = False
        print(f"âŒ Pipeline validation failed. Results: {validation_results}")
        raise Exception("Pipeline validation failed")
    
    print("ðŸŽ‰ Pre-deployment validation successful!")
    return validation_results

# Task ì •ì˜
generate_data_task = PythonOperator(
    task_id='generate_sample_data',
    python_callable=generate_sample_data,
    outlets=[training_data_dataset],
    dag=dag,
)

train_task = PythonOperator(
    task_id='trigger_training',
    python_callable=trigger_mlflow_training,
    inlets=[training_data_dataset],
    outlets=[model_dataset],
    dag=dag,
)

get_model_task = PythonOperator(
    task_id='get_model_info',
    python_callable=get_latest_model_info,
    dag=dag,
)

inference_task = PythonOperator(
    task_id='trigger_inference',
    python_callable=trigger_mlflow_inference,
    inlets=[model_dataset],
    outlets=[inference_results_dataset],
    dag=dag,
)

manual_deployment_instructions = BashOperator(
    task_id='manual_deployment_instructions',
    bash_command=(
        "echo '\n' && "
        "echo '################################################################################' && "
        "echo 'âœ… Model training and validation complete.' && "
        "echo 'ðŸ‘‰ To deploy this model with BentoML, please run the following command in your shell:' && "
        "echo '   docker-compose exec airflow bash -c \"python /opt/airflow/scripts/deploy_with_bentoml.py\"' && "
        "echo '################################################################################' && "
        "echo '\n'"
    ),
    dag=dag,
)

validate_task = PythonOperator(
    task_id='validate_pipeline',
    python_callable=validate_pipeline,
    dag=dag,
)

# Task ì˜ì¡´ì„± ì •ì˜
generate_data_task >> train_task >> get_model_task >> inference_task >> validate_task >> manual_deployment_instructions