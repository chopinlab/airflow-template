#!/usr/bin/env python3
"""
MNIST ëª¨ë¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
"""

import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import mlflow
import mlflow.pytorch
import numpy as np
import json
import os
from datetime import datetime
import matplotlib
matplotlib.use('Agg')  # GUI ì—†ëŠ” í™˜ê²½ì—ì„œ ì‚¬ìš©
import matplotlib.pyplot as plt

class MNISTNet(nn.Module):
    """MNISTë¥¼ ìœ„í•œ ê°„ë‹¨í•œ CNN ëª¨ë¸"""
    
    def __init__(self):
        super(MNISTNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)
        
    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

def visualize_predictions(model, device, test_loader, num_samples=10, save_path="predictions.png"):
    """ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
    model.eval()
    
    # ìƒ˜í”Œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    dataiter = iter(test_loader)
    images, labels = next(dataiter)
    
    # ì˜ˆì¸¡
    with torch.no_grad():
        images = images.to(device)
        outputs = model(images)
        predictions = outputs.argmax(dim=1)
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 5, figsize=(12, 6))
    axes = axes.ravel()
    
    for i in range(min(num_samples, len(images))):
        img = images[i].cpu().squeeze()
        pred = predictions[i].cpu().item()
        true = labels[i].item()
        
        axes[i].imshow(img, cmap='gray')
        color = 'green' if pred == true else 'red'
        axes[i].set_title(f'Pred: {pred}, True: {true}', color=color)
        axes[i].axis('off')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    return save_path

def run_inference(model, device, test_loader):
    """ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ì— ëŒ€í•œ ì¶”ë¡  ì‹¤í–‰"""
    model.eval()
    
    all_predictions = []
    all_labels = []
    all_probabilities = []
    
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(test_loader):
            data, target = data.to(device), target.to(device)
            
            outputs = model(data)
            probabilities = torch.exp(outputs)  # log_softmax -> softmax
            predictions = outputs.argmax(dim=1)
            
            # ë°°ì¹˜ ê²°ê³¼ ìˆ˜ì§‘
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(target.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())
            
            # ì •í™•ë„ ê³„ì‚°
            correct += predictions.eq(target).sum().item()
            total += target.size(0)
            
            if batch_idx % 10 == 0:
                print(f'Inference progress: {batch_idx * len(data)}/{len(test_loader.dataset)} '
                      f'({100. * batch_idx / len(test_loader):.0f}%)')
    
    accuracy = 100. * correct / total
    
    return {
        'predictions': all_predictions,
        'labels': all_labels,
        'probabilities': all_probabilities,
        'accuracy': accuracy,
        'total_samples': total,
        'correct_samples': correct
    }

def calculate_class_metrics(predictions, labels, num_classes=10):
    """í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
    
    precision, recall, f1, support = precision_recall_fscore_support(
        labels, predictions, average=None, labels=range(num_classes)
    )
    
    cm = confusion_matrix(labels, predictions, labels=range(num_classes))
    
    class_metrics = {}
    for i in range(num_classes):
        class_metrics[f'class_{i}'] = {
            'precision': float(precision[i]),
            'recall': float(recall[i]),
            'f1_score': float(f1[i]),
            'support': int(support[i])
        }
    
    return class_metrics, cm.tolist()

def main():
    parser = argparse.ArgumentParser(description='MNIST Model Inference')
    parser.add_argument('--model-uri', type=str, required=True,
                        help='MLflow model URI')
    parser.add_argument('--data-dir', type=str, default='./data',
                        help='directory containing MNIST data')
    parser.add_argument('--output-dir', type=str, default='./inference_results',
                        help='directory to save inference results')
    parser.add_argument('--batch-size', type=int, default=1000,
                        help='batch size for inference')
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    device = torch.device("cpu")
    print(f"Using device: {device}")
    
    # MLflow ì„¤ì •
    mlflow.set_tracking_uri("http://mlflow-server:5000")
    
    # ë°ì´í„° ë³€í™˜ ì •ì˜ (í›ˆë ¨ ì‹œì™€ ë™ì¼)
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
    print("Loading MNIST test dataset...")
    test_dataset = datasets.MNIST(args.data_dir, train=False, download=True,
                                  transform=transform)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
    
    print(f"Test samples: {len(test_dataset)}")
    
    try:
        # MLflowì—ì„œ ëª¨ë¸ ë¡œë“œ
        print(f"Loading model from: {args.model_uri}")
        model = mlflow.pytorch.load_model(args.model_uri)
        model.to(device)
        
        print("âœ… Model loaded successfully")
        
        # MLflow ì¶”ë¡  ì‹¤í–‰ ì‹œì‘
        with mlflow.start_run(run_name=f"mnist_inference_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
            
            # ëª¨ë¸ URI ë¡œê¹…
            mlflow.log_param('model_uri', args.model_uri)
            mlflow.log_param('test_samples', len(test_dataset))
            mlflow.log_param('batch_size', args.batch_size)
            
            # ì¶”ë¡  ì‹¤í–‰
            print("ğŸ”® Running inference...")
            results = run_inference(model, device, test_loader)
            
            print(f"ğŸ¯ Inference completed!")
            print(f"Accuracy: {results['accuracy']:.2f}%")
            print(f"Correct: {results['correct_samples']}/{results['total_samples']}")
            
            # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
            print("ğŸ“Š Calculating class-wise metrics...")
            class_metrics, confusion_matrix = calculate_class_metrics(
                results['predictions'], results['labels']
            )
            
            # ì˜ˆì¸¡ ì‹œê°í™”
            viz_path = os.path.join(args.output_dir, 'predictions_visualization.png')
            visualize_predictions(model, device, test_loader, save_path=viz_path)
            
            # ê²°ê³¼ ì €ì¥
            inference_results = {
                'timestamp': datetime.now().isoformat(),
                'model_uri': args.model_uri,
                'overall_accuracy': results['accuracy'],
                'total_samples': results['total_samples'],
                'correct_samples': results['correct_samples'],
                'class_metrics': class_metrics,
                'confusion_matrix': confusion_matrix,
                'sample_predictions': [
                    {
                        'prediction': int(results['predictions'][i]),
                        'label': int(results['labels'][i]),
                        'confidence': float(max(results['probabilities'][i]))
                    }
                    for i in range(min(100, len(results['predictions'])))  # ì²˜ìŒ 100ê°œ ìƒ˜í”Œ
                ]
            }
            
            # JSONìœ¼ë¡œ ê²°ê³¼ ì €ì¥
            results_path = os.path.join(args.output_dir, 'mnist_inference_results.json')
            with open(results_path, 'w') as f:
                json.dump(inference_results, f, indent=2)
            
            print(f"ğŸ“ Results saved to: {results_path}")
            print(f"ğŸ¨ Visualization saved to: {viz_path}")
            
            # MLflowì— ë©”íŠ¸ë¦­ ë¡œê¹…
            mlflow.log_metrics({
                'accuracy': results['accuracy'],
                'total_samples': results['total_samples'],
                'correct_samples': results['correct_samples']
            })
            
            # í´ë˜ìŠ¤ë³„ F1 ìŠ¤ì½”ì–´ ë¡œê¹…
            for class_name, metrics in class_metrics.items():
                mlflow.log_metrics({
                    f'{class_name}_precision': metrics['precision'],
                    f'{class_name}_recall': metrics['recall'],
                    f'{class_name}_f1': metrics['f1_score']
                })
            
            # ì•„í‹°íŒ©íŠ¸ ë¡œê¹…
            mlflow.log_artifact(results_path, "inference_results")
            mlflow.log_artifact(viz_path, "visualizations")
            
            print(f"âœ… Results logged to MLflow")
            print(f"Run ID: {mlflow.active_run().info.run_id}")
            
            return inference_results
            
    except Exception as e:
        print(f"âŒ Error during inference: {str(e)}")
        raise

if __name__ == '__main__':
    main()